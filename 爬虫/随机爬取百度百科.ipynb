{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先用beautifulsoup尝试一下爬取百度百科。  \n",
    "由于百度百科每个词条里面有不定数量的新词条，我们这次采用随机取一个的办法简化过程，先感受以下整个流程。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不管爬取什么网页，首先要做的是观察分析网页html  \n",
    "我们观察百度百科词条的时候发现，它的每一个词条的html里面，其他词条的href都去掉了 https://baike.baidu.com 这个开头。（在跳转时会补全）  \n",
    "因此我们需要考虑将整个链接分成 https://baike.baidu.com 和 爬到的href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen \n",
    "import re \n",
    "import random   #用来生成随机数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里除了爬虫的过程，我们还要做一个循环，让它自动进入下一次词条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 猫 https://baike.baidu.com/item/%E7%8C%AB/22261\n",
      "1 布偶猫 https://baike.baidu.com/item/%E5%B8%83%E5%81%B6%E7%8C%AB\n",
      "2 脊椎动物亚门 https://baike.baidu.com/item/%E8%84%8A%E6%A4%8E%E5%8A%A8%E7%89%A9%E4%BA%9A%E9%97%A8\n",
      "3 爬行纲 https://baike.baidu.com/item/%E7%88%AC%E8%A1%8C%E7%BA%B2\n",
      "4 蛇岛蝮 https://baike.baidu.com/item/%E8%9B%87%E5%B2%9B%E8%9D%AE/7761311\n",
      "5 百度百科：本人词条编辑服务 https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter\n",
      "6 义项 https://baike.baidu.com/item/%E4%B9%89%E9%A1%B9/6176882\n",
      "7 李健 https://baike.baidu.com/item/%E6%9D%8E%E5%81%A5\n",
      "8 心升明月 https://baike.baidu.com/item/%E5%BF%83%E5%8D%87%E6%98%8E%E6%9C%88\n",
      "9 百度百科：本人词条编辑服务 https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://baike.baidu.com'\n",
    "href = ['/item/%E7%8C%AB/22261']    #我这次选的例子是猫\n",
    "#这次我考虑用列表的方式，把每次随机选到的href放入同个列表，再[-1]选取它，实现循环。\n",
    "for i in range(10):    #循环10次\n",
    "    url = base_url + href[-1]\n",
    "    html = urlopen(url).read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    print(i, soup.find('h1').get_text(), url)\n",
    "    \n",
    "    #每一个词条的特征都是在<a>中，且blank（点击跳转）\n",
    "    new_a = soup.find_all('a', {'target': '_blank', 'href': re.compile('/item/.+')})\n",
    "    #这样就找到了词条下面所有的词条的<a>\n",
    "    \n",
    "    #为了防止遇到某一个词条下面没有新词条的情况，我们还要做一个条件判断。若没有新词条，停止。\n",
    "    if len(new_a) != 0:\n",
    "        href.append(random.sample(new_a, 1)[0]['href'])\n",
    "        #单个分组，选取第一组的<a>的href\n",
    "    else:\n",
    "        print('没有新词条')\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
